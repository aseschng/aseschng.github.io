<!DOCTYPE html>
<html lang="en-US">
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2025-09-25T15:40:56+08:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body>
<article class="li"><h6 class="heading">
<span class="type">Item</span><span class="space"> </span><span class="codenumber">1</span><span class="period">.</span>
</h6>
<code class="code-inline tex2jax_ignore">ASR and LLM</code><ol class="lower-alpha">
<li>Speech LLM research with AISG SeaLion team <a class="external" href="https://sea-lion.ai/" target="_blank"><code class="code-inline tex2jax_ignore">Speech SeaLion</code></a>and StepFun team <a class="external" href="https://arxiv.org/abs/2507.16632" target="_blank"><code class="code-inline tex2jax_ignore">Step Audio 2</code></a>
</li>
<li>Tranfer Learning: from large trained LLM model for under-resourced languages (Indonesian-English, Malay-English)</li>
<li>Using LLM to improve ASR by generative error correction: see <a class="external" href="https://arxiv.org/pdf/2309.15701.pdf" target="_blank"><code class="code-inline tex2jax_ignore">Hyporadise</code></a>
</li>
<li>Robust Large vocabulary continuous speech recognition: joint end-to-end ASR with speech enhancement module, wave2vec2, speaker extraction</li>
<li>Speech enhancement: speaker extraction, denoising, feature enhancement, overlapping speech extraction</li>
</ol></article><span class="incontext"><a href="speechLab_intro.html#li-7">in-context</a></span>
</body>
</html>
